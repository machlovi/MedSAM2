{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec03123d-0a90-496a-877a-b8f66a0785f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7573caf9-d457-4b8a-a6e1-34e1f18e42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sam2.build_sam import build_sam2_video_predictor_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2469ed25-729d-4d30-9b24-37dce084113a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from training.train import single_node_runner\n",
    "from training.utils.train_utils import register_omegaconf_resolvers\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "185f7ef3-aaf7-4b56-a5c2-4ca63593655d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/gpfs/data/arsoyd01lab/machlm03/Segmentation/MedSAM2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd0a3b1-2f86-4198-8621-34639e966e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_omegaconf_resolvers()\n",
    "cfg_path = \"/gpfs/home/machlm03/Segmentation/MedSAM2/sam2/configs/sam2.1_hiera_tiny512_FLARE_RECIST.yaml\"\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "\n",
    "\n",
    "available_gpus = torch.cuda.device_count()\n",
    "if \"device\" in cfg.trainer.model:\n",
    "    dev = cfg.trainer.model.device\n",
    "    if isinstance(dev, str) and dev.startswith(\"cuda:\"):\n",
    "        idx = int(dev.split(\":\")[1])\n",
    "        if idx >= available_gpus:\n",
    "            cfg.trainer.model.device = \"cuda:0\"  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993b38e0-ba9f-4fcf-9124-872b8de3a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.trainer.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f729315c-6037-4fbe-8125-ac41e826fa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Only one GPU detected — forcing single-process mode\n"
     ]
    }
   ],
   "source": [
    "available_gpus = torch.cuda.device_count()\n",
    "\n",
    "cfg.launcher.gpus_per_node = 1\n",
    "cfg.launcher.num_nodes = 1\n",
    "\n",
    "# cfg.launcher.gpus_per_node = min(cfg.launcher.gpus_per_node, available_gpus)\n",
    "\n",
    "if cfg.launcher.gpus_per_node == 1:\n",
    "    print(\"⚠️ Only one GPU detected — forcing single-process mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b39d3c6-931a-4916-bc94-79e956086671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaseem-machlovi\u001b[0m (\u001b[33mmachlovi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gpfs/data/arsoyd01lab/machlm03/Segmentation/MedSAM2/wandb/run-20250822_121201-rrh29h5t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/machlovi/medsam2-finetune/runs/rrh29h5t' target=\"_blank\">experiment_name</a></strong> to <a href='https://wandb.ai/machlovi/medsam2-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/machlovi/medsam2-finetune' target=\"_blank\">https://wandb.ai/machlovi/medsam2-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/machlovi/medsam2-finetune/runs/rrh29h5t' target=\"_blank\">https://wandb.ai/machlovi/medsam2-finetune/runs/rrh29h5t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/machlovi/medsam2-finetune/runs/rrh29h5t?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x15542f977380>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"medsam2-finetune\",\n",
    "    name=\"fold1\",\n",
    "    config=OmegaConf.to_container(cfg, resolve=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2eb652a-6492-4626-a8fb-76e23d53530b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(cfg.output_dir)\n",
    "# print(cfg.trainer)  # if present\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4264d7c-7377-4139-bb3d-620c6bd72ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-08-22 12:12:10,665 train_utils.py: 108: MACHINE SEED: 1230\n",
      "INFO 2025-08-22 12:12:10,698 train_utils.py: 154: Logging ENV_VARIABLES\n",
      "INFO 2025-08-22 12:12:10,698 train_utils.py: 155: ANACONDA3_ROOT=/gpfs/share/apps/anaconda3/gpu/2023.09\n",
      "BASH_ENV=/cm/local/apps/environment-modules/4.4.0//init/bash\n",
      "BASH_FUNC_create_passwd%%=() {  tr -cd 'a-zA-Z0-9' < /dev/urandom 2> /dev/null | head -c${1:-8}\n",
      "}\n",
      "BASH_FUNC_find_port%%=() {  local host=\"${1:-localhost}\";\n",
      " local port=$(random_number \"${2:-2000}\" \"${3:-65535}\");\n",
      " while port_used \"${host}:${port}\"; do\n",
      " port=$(random_number \"${2:-2000}\" \"${3:-65535}\");\n",
      " done;\n",
      " echo \"${port}\"\n",
      "}\n",
      "BASH_FUNC_module%%=() {  unset _mlshdbg;\n",
      " if [ \"${MODULES_SILENT_SHELL_DEBUG:-0}\" = '1' ]; then\n",
      " case \"$-\" in \n",
      " *v*x*)\n",
      " set +vx;\n",
      " _mlshdbg='vx'\n",
      " ;;\n",
      " *v*)\n",
      " set +v;\n",
      " _mlshdbg='v'\n",
      " ;;\n",
      " *x*)\n",
      " set +x;\n",
      " _mlshdbg='x'\n",
      " ;;\n",
      " *)\n",
      " _mlshdbg=''\n",
      " ;;\n",
      " esac;\n",
      " fi;\n",
      " unset _mlre _mlIFS;\n",
      " if [ -n \"${IFS+x}\" ]; then\n",
      " _mlIFS=$IFS;\n",
      " fi;\n",
      " IFS=' ';\n",
      " for _mlv in ${MODULES_RUN_QUARANTINE:-};\n",
      " do\n",
      " if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\n",
      " if [ -n \"`eval 'echo ${'$_mlv'+x}'`\" ]; then\n",
      " _mlre=\"${_mlre:-}${_mlv}_modquar='`eval 'echo ${'$_mlv'}'`' \";\n",
      " fi;\n",
      " _mlrv=\"MODULES_RUNENV_${_mlv}\";\n",
      " _mlre=\"${_mlre:-}${_mlv}='`eval 'echo ${'$_mlrv':-}'`' \";\n",
      " fi;\n",
      " done;\n",
      " if [ -n \"${_mlre:-}\" ]; then\n",
      " eval `eval ${_mlre}/usr/bin/tclsh /cm/local/apps/environment-modules/4.4.0/libexec/modulecmd.tcl bash '\"$@\"'`;\n",
      " else\n",
      " eval `/usr/bin/tclsh /cm/local/apps/environment-modules/4.4.0/libexec/modulecmd.tcl bash \"$@\"`;\n",
      " fi;\n",
      " _mlstatus=$?;\n",
      " if [ -n \"${_mlIFS+x}\" ]; then\n",
      " IFS=$_mlIFS;\n",
      " else\n",
      " unset IFS;\n",
      " fi;\n",
      " unset _mlre _mlv _mlrv _mlIFS;\n",
      " if [ -n \"${_mlshdbg:-}\" ]; then\n",
      " set -$_mlshdbg;\n",
      " fi;\n",
      " unset _mlshdbg;\n",
      " return $_mlstatus\n",
      "}\n",
      "BASH_FUNC_module()=() { eval `/usr/bin/tclsh /cm/local/apps/environment-modules/4.4.0/libexec/modulecmd.tcl bash \"$@\"`; }\n",
      "BASH_FUNC_port_used%%=() {  local port=\"${1#*:}\";\n",
      " local host=$((expr \"${1}\" : '\\(.*\\):' || echo \"localhost\") | awk 'END{print $NF}');\n",
      " local port_strategies=(port_used_nc port_used_lsof port_used_bash port_used_python port_used_python3);\n",
      " for strategy in ${port_strategies[@]};\n",
      " do\n",
      " $strategy $host $port;\n",
      " status=$?;\n",
      " if [[ \"$status\" == \"0\" ]] || [[ \"$status\" == \"1\" ]]; then\n",
      " return $status;\n",
      " fi;\n",
      " done;\n",
      " return 127\n",
      "}\n",
      "BASH_FUNC_random_number%%=() {  shuf -i ${1}-${2} -n 1\n",
      "}\n",
      "BASH_FUNC_source_helpers%%=() {  function random_number () \n",
      " { \n",
      " shuf -i ${1}-${2} -n 1\n",
      " };\n",
      " export -f random_number;\n",
      " function port_used_python () \n",
      " { \n",
      " python -c \"import socket; socket.socket().connect(('$1',$2))\" > /dev/null 2>&1\n",
      " };\n",
      " function port_used_python3 () \n",
      " { \n",
      " python3 -c \"import socket; socket.socket().connect(('$1',$2))\" > /dev/null 2>&1\n",
      " };\n",
      " function port_used_nc () \n",
      " { \n",
      " nc -w 2 \"$1\" \"$2\" < /dev/null > /dev/null 2>&1\n",
      " };\n",
      " function port_used_lsof () \n",
      " { \n",
      " lsof -i :\"$2\" > /dev/null 2>&1\n",
      " };\n",
      " function port_used_bash () \n",
      " { \n",
      " local bash_supported=$(strings /bin/bash 2>/dev/null | grep tcp);\n",
      " if [ \"$bash_supported\" == \"/dev/tcp/*/*\" ]; then\n",
      " ( : < /dev/tcp/$1/$2 ) > /dev/null 2>&1;\n",
      " else\n",
      " return 127;\n",
      " fi\n",
      " };\n",
      " function port_used () \n",
      " { \n",
      " local port=\"${1#*:}\";\n",
      " local host=$((expr \"${1}\" : '\\(.*\\):' || echo \"localhost\") | awk 'END{print $NF}');\n",
      " local port_strategies=(port_used_nc port_used_lsof port_used_bash port_used_python port_used_python3);\n",
      " for strategy in ${port_strategies[@]};\n",
      " do\n",
      " $strategy $host $port;\n",
      " status=$?;\n",
      " if [[ \"$status\" == \"0\" ]] || [[ \"$status\" == \"1\" ]]; then\n",
      " return $status;\n",
      " fi;\n",
      " done;\n",
      " return 127\n",
      " };\n",
      " export -f port_used;\n",
      " function find_port () \n",
      " { \n",
      " local host=\"${1:-localhost}\";\n",
      " local port=$(random_number \"${2:-2000}\" \"${3:-65535}\");\n",
      " while port_used \"${host}:${port}\"; do\n",
      " port=$(random_number \"${2:-2000}\" \"${3:-65535}\");\n",
      " done;\n",
      " echo \"${port}\"\n",
      " };\n",
      " export -f find_port;\n",
      " function wait_until_port_used () \n",
      " { \n",
      " local port=\"${1}\";\n",
      " local time=\"${2:-30}\";\n",
      " for ((i=1; i<=time*2; i++))\n",
      " do\n",
      " port_used \"${port}\";\n",
      " port_status=$?;\n",
      " if [ \"$port_status\" == \"0\" ]; then\n",
      " return 0;\n",
      " else\n",
      " if [ \"$port_status\" == \"127\" ]; then\n",
      " echo \"commands to find port were either not found or inaccessible.\";\n",
      " echo \"command options are lsof, nc, bash's /dev/tcp, or python (or python3) with socket lib.\";\n",
      " return 127;\n",
      " fi;\n",
      " fi;\n",
      " sleep 0.5;\n",
      " done;\n",
      " return 1\n",
      " };\n",
      " export -f wait_until_port_used;\n",
      " function create_passwd () \n",
      " { \n",
      " tr -cd 'a-zA-Z0-9' < /dev/urandom 2> /dev/null | head -c${1:-8}\n",
      " };\n",
      " export -f create_passwd\n",
      "}\n",
      "BASH_FUNC_switchml%%=() {  typeset swfound=1;\n",
      " if [ \"${MODULES_USE_COMPAT_VERSION:-0}\" = '1' ]; then\n",
      " typeset swname='main';\n",
      " if [ -e /cm/local/apps/environment-modules/4.4.0//libexec/modulecmd.tcl ]; then\n",
      " typeset swfound=0;\n",
      " unset MODULES_USE_COMPAT_VERSION;\n",
      " fi;\n",
      " else\n",
      " typeset swname='compatibility';\n",
      " if [ -e /cm/local/apps/environment-modules/4.4.0//libexec/modulecmd-compat ]; then\n",
      " typeset swfound=0;\n",
      " MODULES_USE_COMPAT_VERSION=1;\n",
      " export MODULES_USE_COMPAT_VERSION;\n",
      " fi;\n",
      " fi;\n",
      " if [ $swfound -eq 0 ]; then\n",
      " echo \"Switching to Modules $swname version\";\n",
      " source /cm/local/apps/environment-modules/4.4.0//init/bash;\n",
      " else\n",
      " echo \"Cannot switch to Modules $swname version, command not found\";\n",
      " return 1;\n",
      " fi\n",
      "}\n",
      "BASH_FUNC_wait_until_port_used%%=() {  local port=\"${1}\";\n",
      " local time=\"${2:-30}\";\n",
      " for ((i=1; i<=time*2; i++))\n",
      " do\n",
      " port_used \"${port}\";\n",
      " port_status=$?;\n",
      " if [ \"$port_status\" == \"0\" ]; then\n",
      " return 0;\n",
      " else\n",
      " if [ \"$port_status\" == \"127\" ]; then\n",
      " echo \"commands to find port were either not found or inaccessible.\";\n",
      " echo \"command options are lsof, nc, bash's /dev/tcp, or python (or python3) with socket lib.\";\n",
      " return 127;\n",
      " fi;\n",
      " fi;\n",
      " sleep 0.5;\n",
      " done;\n",
      " return 1\n",
      "}\n",
      "BASH_FUNC_which%%=() {  ( alias;\n",
      " eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\n",
      "}\n",
      "CLICOLOR=1\n",
      "CLICOLOR_FORCE=1\n",
      "CONDA_BACKUP_JAVA_HOME=:-\n",
      "CONDA_BACKUP_JAVA_LD_LIBRARY_PATH=:-\n",
      "CONDA_DEFAULT_ENV=base\n",
      "CONDA_EXE=/gpfs/share/apps/anaconda3/gpu/2023.09/bin/conda\n",
      "CONDA_PREFIX=/gpfs/share/apps/anaconda3/gpu/2023.09\n",
      "CONDA_PROMPT_MODIFIER=(base) \n",
      "CONDA_PYTHON_EXE=/gpfs/share/apps/anaconda3/gpu/2023.09/bin/python\n",
      "CONDA_SHLVL=1\n",
      "CONFIG_FILE=/gpfs/home/machlm03/ondemand/data/sys/dashboard/batch_connect/sys/jupyter/output/5e33a2f5-929a-4be1-b8a1-32375d894f61/config.py\n",
      "CPATH=/gpfs/share/apps/anaconda3/gpu/2023.09/include:/gpfs/share/apps/standard-tools/1.0/include:/cm/shared/apps/slurm/current/include:/gpfs/share/apps/json-c/0.17/include\n",
      "CPATH_modshare=/gpfs/share/apps/json-c/0.17/include:1:/gpfs/share/apps/anaconda3/gpu/2023.09/include:1:/gpfs/share/apps/standard-tools/1.0/include:1:/cm/shared/apps/slurm/current/include:1\n",
      "CUDA_CACHE_DISABLE=1\n",
      "CUDA_CMLOCAL_ROOT=/gpfs/share/apps/cuda/11.8\n",
      "CUDA_CUDART_LIBRARY=/gpfs/share/apps/cuda/11.8/targets/x86_64-linux/lib\n",
      "CUDA_CUDART_LIBRARY_modshare=/gpfs/share/apps/cuda/11.8/targets/x86_64-linux/lib:1\n",
      "CUDA_HOME=/gpfs/share/apps/cuda/11.8\n",
      "CUDA_INCLUDE_DIRS=/gpfs/share/apps/cuda/11.8/include\n",
      "CUDA_INCLUDE_DIRS_modshare=/gpfs/share/apps/cuda/11.8/include:1\n",
      "CUDA_INC_PATH=/gpfs/share/apps/cuda/11.8/include\n",
      "CUDA_INC_PATH_modshare=/gpfs/share/apps/cuda/11.8/include:1\n",
      "CUDA_INSTALL_PATH=/gpfs/share/apps/cuda/11.8\n",
      "CUDA_MODULE_LOADING=LAZY\n",
      "CUDA_PATH=/gpfs/share/apps/cuda/11.8\n",
      "CUDA_ROOT=/gpfs/share/apps/cuda/11.8\n",
      "CUDA_VISIBLE_DEVICES=0,1\n",
      "ENABLE_LMOD=0\n",
      "ENV=/cm/local/apps/environment-modules/4.4.0//init/profile.sh\n",
      "ENVIRONMENT=BATCH\n",
      "FORCE_COLOR=1\n",
      "GIT_PAGER=cat\n",
      "GPU_DEVICE_ORDINAL=0,1\n",
      "HISTCONTROL=ignoredups\n",
      "HISTSIZE=1000\n",
      "HOME=/gpfs/home/machlm03\n",
      "HOSTNAME=a100-4038\n",
      "HYDRA_BOOTSTRAP=slurm\n",
      "HYDRA_FULL_ERROR=1\n",
      "HYDRA_LAUNCHER_EXTRA_ARGS=--external-launcher\n",
      "INCLUDEPATH=/gpfs/share/apps/cuda/11.8/extras/Debugger/include:/gpfs/share/apps/cuda/11.8/extras/CUPTI/include:/gpfs/share/apps/cuda/11.8/targets/x86_64-linux/include\n",
      "INCLUDEPATH_modshare=/gpfs/share/apps/cuda/11.8/extras/Debugger/include:1:/gpfs/share/apps/cuda/11.8/extras/CUPTI/include:1:/gpfs/share/apps/cuda/11.8/targets/x86_64-linux/include:1\n",
      "I_MPI_HYDRA_BOOTSTRAP=slurm\n",
      "I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS=--external-launcher\n",
      "JAVA_HOME=/gpfs/share/apps/anaconda3/gpu/2023.09\n",
      "JAVA_LD_LIBRARY_PATH=/gpfs/share/apps/anaconda3/gpu/2023.09/lib/server\n",
      "JPY_PARENT_PID=3309752\n",
      "JPY_SESSION_NAME=/gpfs/home/machlm03/Segmentation/MedSAM2/MSAM2_OAI_trainer.ipynb\n",
      "LANG=en_US.UTF-8\n",
      "LD_LIBRARY_PATH=/gpfs/share/apps/cuda/11.8/extras/CUPTI/lib64:/gpfs/share/apps/cuda/11.8/targets/x86_64-linux/lib:/gpfs/share/apps/cuda/11.8/lib64:/cm/shared/apps/slurm/current/lib64/perl5:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib/perl5:/cm/shared/apps/slurm/current/lib/slurm:/cm/shared/apps/slurm/current/lib64:/cm/shared/apps/slurm/current/lib:/gpfs/share/apps/json-c/0.17/lib64\n",
      "LD_LIBRARY_PATH_modshare=/cm/shared/apps/slurm/current/lib/slurm:1:/cm/shared/apps/slurm/current/lib64:1:/gpfs/share/apps/cuda/11.8/extras/CUPTI/lib64:1:/gpfs/share/apps/cuda/11.8/targets/x86_64-linux/lib:1:/cm/shared/apps/slurm/current/lib/perl5:1:/cm/shared/apps/slurm/current/lib64/slurm:1:/cm/shared/apps/slurm/current/lib:1:/gpfs/share/apps/json-c/0.17/lib64:1:/gpfs/share/apps/cuda/11.8/lib64:1:/cm/shared/apps/slurm/current/lib64/perl5:1\n",
      "LESSOPEN=||/usr/bin/lesspipe.sh %s\n",
      "LIBRARY_PATH=/gpfs/share/apps/cuda/11.8/targets/x86_64-linux/lib:/gpfs/share/apps/cuda/11.8/lib64:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
      "LIBRARY_PATH_modshare=/cm/shared/apps/slurm/current/lib64:1:/gpfs/share/apps/cuda/11.8/targets/x86_64-linux/lib:1:/cm/shared/apps/slurm/current/lib64/slurm:1:/gpfs/share/apps/cuda/11.8/lib64:1\n",
      "LOADEDMODULES=json-c/0.17:slurm/current:standard-tools/1.0:default-environment:cuda/11.8:anaconda3/gpu/new\n",
      "LOADEDMODULES_modshare=anaconda3/gpu/new:1:cuda/11.8:1:slurm/current:1:standard-tools/1.0:1:json-c/0.17:1:default-environment:1\n",
      "LOCAL_RANK=0\n",
      "LOGNAME=machlm03\n",
      "MAIL=/var/spool/mail/machlm03\n",
      "MANPATH=/gpfs/share/apps/anaconda3/gpu/2023.09/man:/gpfs/share/apps/cuda/11.8/doc/man:/gpfs/share/apps/standard-tools/1.0/share/man:/cm/shared/apps/slurm/current/share/man:/cm/local/apps/environment-modules/4.4.0//share/man:/usr/local/share/man:/usr/share/man:/cm/local/apps/environment-modules/current/share/man\n",
      "MANPATH_modshare=/usr/local/share/man:1:/gpfs/share/apps/standard-tools/1.0/share/man:1:/cm/shared/apps/slurm/current/share/man:1:/cm/local/apps/environment-modules/4.4.0//share/man:1:/cm/local/apps/environment-modules/current/share/man:1:/gpfs/share/apps/anaconda3/gpu/2023.09/man:1:/usr/share/man:1:/gpfs/share/apps/cuda/11.8/doc/man:1\n",
      "MASTER_ADDR=127.0.0.1\n",
      "MASTER_PORT=12355\n",
      "MODULEPATH=/cm/shared/modulefiles\n",
      "MODULESHOME=/cm/local/apps/environment-modules/4.4.0/\n",
      "MODULES_CMD=/cm/local/apps/environment-modules/4.4.0/libexec/modulecmd.tcl\n",
      "MODULES_LMALTNAME=slurm/current&slurm/default&slurm\n",
      "MODULES_LMALTNAME_modshare=slurm/current&slurm/default&slurm:1\n",
      "MODULES_LMCONFLICT=anaconda3/gpu/new&anaconda3&python\n",
      "MODULES_LMCONFLICT_modshare=anaconda3/gpu/new&anaconda3&python:1\n",
      "MODULES_LMNOTUASKED=json-c/0.17:slurm/current:standard-tools/1.0:cuda/11.8\n",
      "MODULES_LMNOTUASKED_modshare=cuda/11.8:1:slurm/current:1:standard-tools/1.0:1:json-c/0.17:1\n",
      "MODULES_LMPREREQ=slurm/current&json-c/0.17:default-environment&slurm&standard-tools/1.0:anaconda3/gpu/new&cuda/11.8&cuda\n",
      "MODULES_LMPREREQ_modshare=anaconda3/gpu/new&cuda/11.8&cuda:1:slurm/current&json-c/0.17:1:default-environment&slurm&standard-tools/1.0:1\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "OLDPWD=/gpfs/home/machlm03/ondemand/data/sys/dashboard/batch_connect/sys/jupyter/output/5e33a2f5-929a-4be1-b8a1-32375d894f61\n",
      "OMPI_MCA_plm_slurm_args=--external-launcher\n",
      "PAGER=cat\n",
      "PATH=/gpfs/share/apps/cuda/11.8/bin:/gpfs/share/apps/standard-tools/1.0/bin:/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/cm/local/apps/environment-modules/4.4.0//bin:/gpfs/share/apps/anaconda3/gpu/2023.09/bin:/gpfs/share/apps/anaconda3/gpu/2023.09/condabin:/cm/local/apps/environment-modules/4.4.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/sbin:/cm/local/apps/environment-modules/current/bin\n",
      "PATH_modshare=/cm/local/apps/environment-modules/current/bin:1:/gpfs/share/apps/standard-tools/1.0/bin:1:/cm/local/apps/environment-modules/4.4.0//bin:1:/gpfs/share/apps/anaconda3/gpu/2023.09/condabin:1:/usr/bin:1:/usr/local/bin:1:/cm/shared/apps/slurm/current/bin:1:/cm/shared/apps/slurm/current/sbin:1:/cm/local/apps/environment-modules/4.4.0/bin:1:/sbin:1:/gpfs/share/apps/anaconda3/gpu/2023.09/bin:2:/usr/sbin:1:/usr/local/sbin:1:/gpfs/share/apps/cuda/11.8/bin:1\n",
      "PERL5LIB=/cm/shared/apps/slurm/current/lib64/perl5\n",
      "PERL5LIB_modshare=/cm/shared/apps/slurm/current/lib64/perl5:1\n",
      "PKG_CONFIG_PATH=/gpfs/share/apps/json-c/0.17/lib64/pkgconfig\n",
      "PKG_CONFIG_PATH_modshare=/gpfs/share/apps/json-c/0.17/lib64/pkgconfig:1\n",
      "PRTE_MCA_plm_slurm_args=--external-launcher\n",
      "PWD=/gpfs/home/machlm03\n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "PYTHONPATH=/gpfs/share/apps/anaconda3/gpu/2023.09/lib/python*/site-packages\n",
      "PYTHONPATH_modshare=/gpfs/share/apps/anaconda3/gpu/2023.09/lib/python*/site-packages:1\n",
      "RANK=0\n",
      "ROCR_VISIBLE_DEVICES=0,1\n",
      "SHELL=/bin/bash\n",
      "SHLVL=3\n",
      "SLURMD_NODENAME=a100-4038\n",
      "SLURM_CLUSTER_NAME=slurm_cluster\n",
      "SLURM_CONF=/etc/slurm/slurm.conf\n",
      "SLURM_CPUS_ON_NODE=16\n",
      "SLURM_CPUS_PER_TASK=16\n",
      "SLURM_EXPORT_ENV=NONE\n",
      "SLURM_GET_USER_ENV=1\n",
      "SLURM_GPUS_ON_NODE=2\n",
      "SLURM_GTIDS=0\n",
      "SLURM_JOBID=5686046\n",
      "SLURM_JOB_ACCOUNT=system\n",
      "SLURM_JOB_CPUS_PER_NODE=16\n",
      "SLURM_JOB_END_TIME=1756045886\n",
      "SLURM_JOB_GID=1024353785\n",
      "SLURM_JOB_GPUS=0,2\n",
      "SLURM_JOB_ID=5686046\n",
      "SLURM_JOB_NAME=MedSAM_training\n",
      "SLURM_JOB_NODELIST=a100-4038\n",
      "SLURM_JOB_NUM_NODES=1\n",
      "SLURM_JOB_PARTITION=a100_long\n",
      "SLURM_JOB_QOS=normal\n",
      "SLURM_JOB_START_TIME=1755873086\n",
      "SLURM_JOB_UID=1024344004\n",
      "SLURM_JOB_USER=machlm03\n",
      "SLURM_LOCALID=0\n",
      "SLURM_MEM_PER_NODE=40960\n",
      "SLURM_NNODES=1\n",
      "SLURM_NODEID=0\n",
      "SLURM_NODELIST=a100-4038\n",
      "SLURM_NPROCS=1\n",
      "SLURM_NTASKS=1\n",
      "SLURM_PRIO_PROCESS=0\n",
      "SLURM_PROCID=0\n",
      "SLURM_SCRIPT_CONTEXT=prolog_task\n",
      "SLURM_SUBMIT_DIR=/var/www/ood/apps/sys/dashboard\n",
      "SLURM_SUBMIT_HOST=an-0003\n",
      "SLURM_TASKS_PER_NODE=1\n",
      "SLURM_TASK_PID=3309588\n",
      "SLURM_TOPOLOGY_ADDR=a100-4038\n",
      "SLURM_TOPOLOGY_ADDR_PATTERN=node\n",
      "SLURM_TRES_PER_TASK=cpu:16\n",
      "S_COLORS=auto\n",
      "TERM=xterm-color\n",
      "TMPDIR=/tmp\n",
      "TORCH_NCCL_ASYNC_ERROR_HANDLING=1\n",
      "USER=machlm03\n",
      "WANDB_SERVICE=3-3322671-unix-/tmp/wandb-3322671-3322777-2380660957/socket\n",
      "WORLD_SIZE=1\n",
      "XDG_RUNTIME_DIR=/tmp/1024344004\n",
      "ZE_AFFINITY_MASK=0,1\n",
      "_=/gpfs/share/apps/anaconda3/gpu/2023.09/bin/jupyter\n",
      "_CE_CONDA=\n",
      "_CE_M=\n",
      "_LMFILES_=/cm/shared/modulefiles/json-c/0.17:/cm/shared/modulefiles/slurm/current:/cm/shared/modulefiles/standard-tools/1.0:/cm/shared/modulefiles/default-environment:/cm/shared/modulefiles/cuda/11.8:/cm/shared/modulefiles/anaconda3/gpu/new\n",
      "_LMFILES__modshare=/cm/shared/modulefiles/anaconda3/gpu/new:1:/cm/shared/modulefiles/cuda/11.8:1:/cm/shared/modulefiles/slurm/current:1:/cm/shared/modulefiles/standard-tools/1.0:1:/cm/shared/modulefiles/json-c/0.17:1:/cm/shared/modulefiles/default-environment:1\n",
      "host=a100-4038.ib.cluster\n",
      "port=23328\n",
      "which_declare=declare -f\n",
      "\n",
      "INFO 2025-08-22 12:12:10,700 trainer.py:1017: Setting up components: Model, loss, optim, meters etc.\n",
      "INFO 2025-08-22 12:12:10,701 logger.py:  86: TensorBoard SummaryWriter instantiated. Files will be stored in: exp_log/tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_name</strong> at: <a href='https://wandb.ai/machlovi/medsam2-finetune/runs/rrh29h5t' target=\"_blank\">https://wandb.ai/machlovi/medsam2-finetune/runs/rrh29h5t</a><br> View project at: <a href='https://wandb.ai/machlovi/medsam2-finetune' target=\"_blank\">https://wandb.ai/machlovi/medsam2-finetune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250822_121201-rrh29h5t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>exp_log/logs/wandb/run-20250822_121210-u2mwtanj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/machlovi/medsam2-finetune/runs/u2mwtanj' target=\"_blank\">exp_20250822_121210</a></strong> to <a href='https://wandb.ai/machlovi/medsam2-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/machlovi/medsam2-finetune' target=\"_blank\">https://wandb.ai/machlovi/medsam2-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/machlovi/medsam2-finetune/runs/u2mwtanj' target=\"_blank\">https://wandb.ai/machlovi/medsam2-finetune/runs/u2mwtanj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-08-22 12:12:12,524 sam2.py:  81: Training with points (sampled from masks) as inputs with p=0.5\n",
      "INFO 2025-08-22 12:12:12,526 trainer.py:1106: ====================\n",
      "INFO 2025-08-22 12:12:12,526 trainer.py:1107: Summary for model <class 'training.model.sam2.SAM2Train'>\n",
      "INFO 2025-08-22 12:12:12,528 trainer.py:1108: Model is SAM2Train(\n",
      "  (image_encoder): ImageEncoder(\n",
      "    (trunk): Hiera(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
      "      )\n",
      "      (blocks): ModuleList(\n",
      "        (0): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (1): Linear(in_features=384, out_features=96, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "            (qkv): Linear(in_features=96, out_features=576, bias=True)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "          (proj): Linear(in_features=96, out_features=192, bias=True)\n",
      "        )\n",
      "        (2): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (3): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "            (qkv): Linear(in_features=192, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (1): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "          (proj): Linear(in_features=192, out_features=384, bias=True)\n",
      "        )\n",
      "        (4-9): 6 x MultiScaleBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (1): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (10): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "            (qkv): Linear(in_features=384, out_features=2304, bias=True)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (1): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "          (proj): Linear(in_features=384, out_features=768, bias=True)\n",
      "        )\n",
      "        (11): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (1): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (neck): FpnNeck(\n",
      "      (position_encoding): PositionEmbeddingSine()\n",
      "      (convs): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (conv): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n",
      "  (memory_attention): MemoryAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x MemoryAttentionLayer(\n",
      "        (self_attn): RoPEAttention(\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (cross_attn_image): RoPEAttention(\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (k_proj): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (memory_encoder): MemoryEncoder(\n",
      "    (mask_downsampler): MaskDownSampler(\n",
      "      (encoder): Sequential(\n",
      "        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): LayerNorm2d()\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (4): LayerNorm2d()\n",
      "        (5): GELU(approximate='none')\n",
      "        (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (7): LayerNorm2d()\n",
      "        (8): GELU(approximate='none')\n",
      "        (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (10): LayerNorm2d()\n",
      "        (11): GELU(approximate='none')\n",
      "        (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fuser): Fuser(\n",
      "      (proj): Identity()\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x CXBlock(\n",
      "          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm2d()\n",
      "          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (position_encoding): PositionEmbeddingSine()\n",
      "    (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (sam_prompt_encoder): PromptEncoder(\n",
      "    (pe_layer): PositionEmbeddingRandom()\n",
      "    (point_embeddings): ModuleList(\n",
      "      (0-3): 4 x Embedding(1, 256)\n",
      "    )\n",
      "    (not_a_point_embed): Embedding(1, 256)\n",
      "    (mask_downscaling): Sequential(\n",
      "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): LayerNorm2d()\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (4): LayerNorm2d()\n",
      "      (5): GELU(approximate='none')\n",
      "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (no_mask_embed): Embedding(1, 256)\n",
      "  )\n",
      "  (sam_mask_decoder): MaskDecoder(\n",
      "    (transformer): TwoWayTransformer(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TwoWayAttentionBlock(\n",
      "          (self_attn): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (cross_attn_token_to_image): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (1): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            )\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (cross_attn_image_to_token): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_attn_token_to_image): Attention(\n",
      "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (iou_token): Embedding(1, 256)\n",
      "    (mask_tokens): Embedding(4, 256)\n",
      "    (obj_score_token): Embedding(1, 256)\n",
      "    (output_upscaling): Sequential(\n",
      "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): LayerNorm2d()\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (4): GELU(approximate='none')\n",
      "    )\n",
      "    (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (output_hypernetworks_mlps): ModuleList(\n",
      "      (0-3): 4 x MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "        )\n",
      "        (act): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (iou_prediction_head): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (pred_obj_score_head): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "      )\n",
      "      (act): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (obj_ptr_proj): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (act): ReLU()\n",
      "  )\n",
      "  (obj_ptr_tpos_proj): Linear(in_features=256, out_features=64, bias=True)\n",
      ")\n",
      "INFO 2025-08-22 12:12:12,529 trainer.py:1109: \tTotal parameters 39.0 M\n",
      "INFO 2025-08-22 12:12:12,529 trainer.py:1110: \tTrainable parameters 39.0 M\n",
      "INFO 2025-08-22 12:12:12,529 trainer.py:1113: \tNon-Trainable parameters 0  \n",
      "INFO 2025-08-22 12:12:12,530 trainer.py:1116: ====================\n",
      "INFO 2025-08-22 12:12:12,533 trainer.py:1075: Finished setting up components: Model, loss, optim, meters etc.\n",
      "INFO 2025-08-22 12:12:12,533 trainer.py: 353: Moving components to device cuda:0 and local rank 0.\n",
      "INFO 2025-08-22 12:12:12,696 trainer.py: 357: Done moving components to device cuda:0 and local rank 0.\n",
      "INFO 2025-08-22 12:12:12,706 optimizer.py: 248: Matches for param_name [image_encoder.*]: {'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.weight', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.11.attn.proj.weight', 'image_encoder.trunk.blocks.10.proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.weight', 'image_encoder.trunk.blocks.7.attn.proj.weight', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.weight', 'image_encoder.trunk.patch_embed.proj.weight', 'image_encoder.neck.convs.3.conv.weight', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.pos_embed_window', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.weight', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.weight', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.3.attn.proj.weight', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.5.attn.proj.weight', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.neck.convs.0.conv.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.weight', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.9.mlp.layers.1.weight', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.attn.qkv.weight', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.2.attn.qkv.weight', 'image_encoder.trunk.blocks.10.mlp.layers.1.weight', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.3.attn.qkv.weight', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.weight', 'image_encoder.trunk.blocks.11.attn.qkv.weight', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.attn.proj.weight', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.7.mlp.layers.0.weight', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.1.attn.proj.weight', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.10.proj.weight', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.weight', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.10.attn.proj.weight', 'image_encoder.trunk.blocks.1.attn.qkv.weight', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.weight', 'image_encoder.trunk.blocks.10.mlp.layers.0.weight', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.3.proj.bias', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.11.mlp.layers.1.weight', 'image_encoder.trunk.blocks.9.attn.proj.weight', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.neck.convs.1.conv.weight', 'image_encoder.trunk.blocks.6.attn.qkv.weight', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.1.proj.bias', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.4.mlp.layers.1.weight', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.10.attn.qkv.weight', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.3.proj.weight', 'image_encoder.trunk.blocks.0.mlp.layers.0.weight', 'image_encoder.trunk.blocks.0.attn.qkv.weight', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.6.attn.proj.weight', 'image_encoder.trunk.blocks.2.mlp.layers.1.weight', 'image_encoder.trunk.blocks.5.mlp.layers.0.weight', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.8.attn.qkv.weight', 'image_encoder.trunk.pos_embed', 'image_encoder.trunk.blocks.0.attn.proj.weight', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.weight', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.weight', 'image_encoder.neck.convs.2.conv.weight', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.1.proj.weight', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.weight', 'image_encoder.trunk.blocks.6.mlp.layers.0.weight', 'image_encoder.trunk.blocks.4.attn.qkv.weight', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.attn.qkv.weight', 'image_encoder.trunk.blocks.4.attn.proj.weight', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.attn.qkv.weight', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.2.attn.proj.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.weight', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.weight'}\n",
      "INFO 2025-08-22 12:12:12,707 optimizer.py: 248: Matches for param_name [*bias*]: {'image_encoder.trunk.blocks.0.norm1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'memory_attention.layers.3.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.10.proj.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias', 'sam_mask_decoder.output_upscaling.0.bias', 'memory_attention.layers.3.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'memory_encoder.mask_downsampler.encoder.0.bias', 'memory_attention.layers.2.linear1.bias', 'sam_mask_decoder.pred_obj_score_head.layers.1.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'memory_encoder.mask_downsampler.encoder.9.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'memory_encoder.mask_downsampler.encoder.3.bias', 'memory_encoder.fuser.layers.1.pwconv1.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'memory_attention.layers.3.cross_attn_image.k_proj.bias', 'memory_attention.layers.3.cross_attn_image.out_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'image_encoder.neck.convs.1.conv.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'memory_attention.layers.0.cross_attn_image.k_proj.bias', 'memory_attention.layers.1.norm3.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'memory_attention.layers.3.self_attn.out_proj.bias', 'memory_encoder.mask_downsampler.encoder.6.bias', 'memory_attention.layers.0.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'memory_attention.layers.3.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'memory_attention.layers.0.self_attn.k_proj.bias', 'memory_attention.layers.0.cross_attn_image.v_proj.bias', 'image_encoder.neck.convs.3.conv.bias', 'memory_attention.layers.0.norm2.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'memory_attention.layers.3.linear1.bias', 'memory_attention.layers.2.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias', 'sam_prompt_encoder.mask_downscaling.3.bias', 'obj_ptr_tpos_proj.bias', 'sam_mask_decoder.output_upscaling.3.bias', 'memory_encoder.mask_downsampler.encoder.12.bias', 'memory_encoder.mask_downsampler.encoder.7.bias', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'memory_attention.layers.2.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'memory_attention.layers.2.self_attn.k_proj.bias', 'memory_attention.layers.0.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'memory_attention.layers.1.cross_attn_image.q_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'memory_attention.layers.2.norm3.bias', 'memory_attention.layers.1.self_attn.out_proj.bias', 'sam_prompt_encoder.mask_downscaling.4.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'obj_ptr_proj.layers.1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'memory_attention.layers.2.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'memory_encoder.fuser.layers.1.dwconv.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'sam_mask_decoder.iou_prediction_head.layers.2.bias', 'memory_attention.layers.0.linear2.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'memory_attention.layers.1.self_attn.v_proj.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'memory_encoder.fuser.layers.1.pwconv2.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'memory_encoder.fuser.layers.0.dwconv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'memory_attention.layers.2.cross_attn_image.k_proj.bias', 'memory_attention.layers.1.cross_attn_image.k_proj.bias', 'sam_mask_decoder.iou_prediction_head.layers.0.bias', 'image_encoder.trunk.blocks.3.proj.bias', 'memory_encoder.fuser.layers.0.pwconv2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'memory_attention.layers.2.norm2.bias', 'memory_attention.layers.0.norm3.bias', 'memory_attention.layers.1.linear2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'sam_mask_decoder.iou_prediction_head.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'sam_mask_decoder.output_upscaling.1.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.1.proj.bias', 'memory_attention.layers.1.linear1.bias', 'memory_attention.layers.1.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'memory_attention.layers.1.self_attn.q_proj.bias', 'mask_downsample.bias', 'memory_attention.layers.2.self_attn.q_proj.bias', 'memory_attention.layers.3.self_attn.q_proj.bias', 'memory_attention.layers.0.cross_attn_image.out_proj.bias', 'memory_attention.layers.0.self_attn.out_proj.bias', 'memory_attention.layers.3.linear2.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'memory_attention.layers.0.self_attn.q_proj.bias', 'obj_ptr_proj.layers.0.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'memory_attention.layers.3.cross_attn_image.v_proj.bias', 'sam_mask_decoder.pred_obj_score_head.layers.0.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'memory_attention.layers.2.cross_attn_image.v_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'memory_encoder.mask_downsampler.encoder.1.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias', 'sam_mask_decoder.pred_obj_score_head.layers.2.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'memory_encoder.mask_downsampler.encoder.10.bias', 'memory_attention.layers.2.linear2.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'memory_attention.layers.0.linear1.bias', 'memory_attention.layers.3.norm1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'memory_encoder.mask_downsampler.encoder.4.bias', 'memory_attention.layers.1.self_attn.k_proj.bias', 'memory_encoder.fuser.layers.1.norm.bias', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'memory_encoder.pix_feat_proj.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'memory_attention.layers.3.norm2.bias', 'memory_encoder.fuser.layers.0.norm.bias', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'sam_prompt_encoder.mask_downscaling.0.bias', 'memory_attention.layers.1.norm2.bias', 'memory_attention.layers.1.cross_attn_image.out_proj.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'sam_mask_decoder.conv_s1.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias', 'sam_prompt_encoder.mask_downscaling.1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'sam_prompt_encoder.mask_downscaling.6.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'sam_mask_decoder.conv_s0.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'memory_encoder.out_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'memory_encoder.fuser.layers.0.pwconv1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'image_encoder.neck.convs.0.conv.bias', 'memory_attention.layers.1.cross_attn_image.v_proj.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'obj_ptr_proj.layers.2.bias', 'memory_attention.layers.3.norm3.bias', 'memory_attention.layers.2.self_attn.out_proj.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias'}\n",
      "INFO 2025-08-22 12:12:12,707 optimizer.py: 220: Matches for module_cls_name [torch.nn.LayerNorm]: {'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.7.norm1.bias', 'memory_attention.layers.0.norm3.weight', 'image_encoder.trunk.blocks.6.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'memory_attention.layers.2.norm2.weight', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'memory_attention.layers.3.norm2.weight', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'memory_attention.layers.1.norm3.bias', 'sam_mask_decoder.transformer.layers.0.norm4.weight', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'memory_attention.layers.0.norm1.weight', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.6.norm2.weight', 'memory_attention.layers.3.norm3.weight', 'image_encoder.trunk.blocks.9.norm2.weight', 'memory_attention.layers.1.norm3.weight', 'sam_mask_decoder.transformer.layers.1.norm1.weight', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'memory_attention.layers.0.norm2.bias', 'image_encoder.trunk.blocks.4.norm2.weight', 'memory_attention.layers.1.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm4.weight', 'memory_attention.layers.2.norm3.weight', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm2.weight', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.6.norm1.bias', 'memory_attention.layers.2.norm3.bias', 'image_encoder.trunk.blocks.2.norm2.weight', 'memory_attention.layers.0.norm2.weight', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.0.norm2.weight', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'memory_attention.layers.2.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm3.weight', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.3.norm1.weight', 'memory_attention.layers.2.norm2.bias', 'memory_attention.layers.0.norm3.bias', 'image_encoder.trunk.blocks.10.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm2.weight', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.10.norm1.bias', 'memory_attention.layers.1.norm1.bias', 'image_encoder.trunk.blocks.2.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.11.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm1.weight', 'image_encoder.trunk.blocks.1.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'sam_mask_decoder.transformer.norm_final_attn.weight', 'image_encoder.trunk.blocks.8.norm2.bias', 'memory_attention.layers.3.norm1.bias', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.11.norm2.weight', 'memory_attention.layers.1.norm1.weight', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.3.norm2.weight', 'memory_attention.layers.0.norm1.bias', 'memory_attention.layers.1.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'image_encoder.trunk.blocks.10.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm3.weight', 'image_encoder.trunk.blocks.8.norm1.weight', 'memory_attention.layers.2.norm1.weight', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.6.norm1.weight', 'memory_attention.layers.3.norm1.weight', 'memory_attention.norm.weight', 'image_encoder.trunk.blocks.1.norm1.weight', 'memory_attention.layers.3.norm3.bias', 'image_encoder.trunk.blocks.5.norm2.weight'} \n",
      "Raw dataset length = 42\n",
      "dataset_lens: [5]\n",
      "total_len: 5\n",
      "INFO 2025-08-22 12:12:15,444 sam2_datasets.py: 129: Dataset mixing probabilities: [1.0]\n",
      "Raw dataset length = 42\n",
      "dataset_lens: [5]\n",
      "total_len: 5\n",
      "INFO 2025-08-22 12:12:15,499 sam2_datasets.py: 129: Dataset mixing probabilities: [1.0]\n",
      "INFO 2025-08-22 12:12:16,380 trainer.py: 441: Loading pretrained checkpoint from {'_partial_': True, '_target_': 'training.utils.checkpoint_utils.load_state_dict_into_model', 'strict': True, 'ignore_unexpected_keys': None, 'ignore_missing_keys': None, 'state_dict': {'_target_': 'training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels', 'checkpoint_path': 'checkpoints/sam2.1_hiera_tiny.pt', 'ckpt_state_dict_keys': ['model']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/machlm03/.conda/envs/medsam2/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "/gpfs/home/machlm03/.conda/envs/medsam2/lib/python3.12/site-packages/torch/autograd/graph.py:824: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [64, 256, 1, 1], strides() = [256, 1, 256, 256]\n",
      "bucket_view.sizes() = [64, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:328.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-08-22 12:12:34,967 train_utils.py: 271: Train Epoch: [0][0/5] | Batch Time: 18.39 (18.39) | Data Time: 15.50 (15.50) | Mem (GB): 55.00 (55.00/55.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 1.71e+00 (1.71e+00) | Losses/train_vos_loss: 0.00e+00 (0.00e+00)\n",
      "INFO 2025-08-22 12:13:01,085 trainer.py: 983: Estimated time remaining: 00d 00h 06m\n",
      "INFO 2025-08-22 12:13:01,086 trainer.py: 926: Synchronizing meters\n",
      "INFO 2025-08-22 12:13:01,086 trainer.py: 867: Losses and meters: {'Losses/train_all_loss': 3.6105692625045775, 'Losses/train_vos_loss': 0, 'Losses/train_all_loss_mask': 0.09328365568071603, 'Losses/train_all_loss_dice': 1.0642080664634705, 'Losses/train_all_loss_iou': 0.3223196566104889, 'Losses/train_all_loss_class': 0.35836826839949937, 'Losses/train_all_core_loss': 3.6105692625045775, 'Trainer/where': 0.08, 'Trainer/epoch': 0, 'Trainer/steps_train': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 5. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-08-22 12:13:15,107 train_utils.py: 271: Val Epoch: [0][0/5] | Batch Time: 13.47 (13.47) | Data Time: 12.41 (12.41) | Mem (GB): 3.00 (3.00/3.00) | Time Elapsed: 00d 00h 01m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_vos_loss: 4.41e+00 (4.41e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 5. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-08-22 12:13:38,554 trainer.py: 983: Estimated time remaining: 00d 00h 12m\n",
      "INFO 2025-08-22 12:13:38,555 trainer.py: 926: Synchronizing meters\n",
      "INFO 2025-08-22 12:13:38,555 trainer.py: 729: Meters: {'Losses/val_all_loss': 0, 'Losses/val_vos_loss': 3.918986749649048, 'Losses/val_vos_loss_mask': 0.048368234932422635, 'Losses/val_vos_loss_dice': 2.2823318004608155, 'Losses/val_vos_loss_iou': 0.6692900657653809, 'Losses/val_vos_loss_class': 2.2264077585987253e-07, 'Losses/val_vos_core_loss': 3.918986749649048, 'Trainer/where': 0.08, 'Trainer/epoch': 0, 'Trainer/steps_val': 10}\n",
      "INFO 2025-08-22 12:13:38,802 trainer.py: 580: New best val loss: 3.918987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 10. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-08-22 12:13:54,863 train_utils.py: 271: Train Epoch: [1][0/5] | Batch Time: 16.06 (16.06) | Data Time: 14.51 (14.51) | Mem (GB): 41.00 (41.00/41.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 1.77e+01 (1.77e+01) | Losses/train_vos_loss: 0.00e+00 (0.00e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5 that is less than the current step 10. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 6 that is less than the current step 10. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-08-22 12:14:23,049 trainer.py: 983: Estimated time remaining: 00d 00h 11m\n",
      "INFO 2025-08-22 12:14:23,050 trainer.py: 926: Synchronizing meters\n",
      "INFO 2025-08-22 12:14:23,051 trainer.py: 867: Losses and meters: {'Losses/train_all_loss': 6.063936161994934, 'Losses/train_vos_loss': 0, 'Losses/train_all_loss_mask': 0.04866731856018305, 'Losses/train_all_loss_dice': 1.7072018384933472, 'Losses/train_all_loss_iou': 0.5854829609394073, 'Losses/train_all_loss_class': 2.7979046531021594, 'Losses/train_all_core_loss': 6.063936161994934, 'Trainer/where': 0.18, 'Trainer/epoch': 1, 'Trainer/steps_train': 10}\n",
      "INFO 2025-08-22 12:14:36,378 train_utils.py: 271: Val Epoch: [1][0/5] | Batch Time: 12.76 (12.76) | Data Time: 12.36 (12.36) | Mem (GB): 3.00 (3.00/3.00) | Time Elapsed: 00d 00h 02m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_vos_loss: 3.96e+00 (3.96e+00)\n",
      "INFO 2025-08-22 12:15:00,511 trainer.py: 983: Estimated time remaining: 00d 00h 10m\n",
      "INFO 2025-08-22 12:15:00,512 trainer.py: 926: Synchronizing meters\n",
      "INFO 2025-08-22 12:15:00,513 trainer.py: 729: Meters: {'Losses/val_all_loss': 0, 'Losses/val_vos_loss': 3.8032578945159914, 'Losses/val_vos_loss_mask': 0.05113771483302117, 'Losses/val_vos_loss_dice': 2.1482203722000124, 'Losses/val_vos_loss_iou': 0.63228280544281, 'Losses/val_vos_loss_class': 4.0374317791247447e-07, 'Losses/val_vos_core_loss': 3.8032578945159914, 'Trainer/where': 0.18, 'Trainer/epoch': 1, 'Trainer/steps_val': 20}\n",
      "INFO 2025-08-22 12:15:00,741 trainer.py: 580: New best val loss: 3.803258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 20. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-08-22 12:15:16,371 train_utils.py: 271: Train Epoch: [2][0/5] | Batch Time: 15.63 (15.63) | Data Time: 14.47 (14.47) | Mem (GB): 41.00 (41.00/41.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 5.59e-01 (5.59e-01) | Losses/train_vos_loss: 0.00e+00 (0.00e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 20. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11 that is less than the current step 20. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-08-22 12:15:44,311 trainer.py: 983: Estimated time remaining: 00d 00h 09m\n",
      "INFO 2025-08-22 12:15:44,312 trainer.py: 926: Synchronizing meters\n",
      "INFO 2025-08-22 12:15:44,312 trainer.py: 867: Losses and meters: {'Losses/train_all_loss': 3.1727493166923524, 'Losses/train_vos_loss': 0, 'Losses/train_all_loss_mask': 0.05264227855950594, 'Losses/train_all_loss_dice': 1.4970196425914764, 'Losses/train_all_loss_iou': 0.6070266216993332, 'Losses/train_all_loss_class': 0.015857644472271205, 'Losses/train_all_core_loss': 3.1727493166923524, 'Trainer/where': 0.27999999999999997, 'Trainer/epoch': 2, 'Trainer/steps_train': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15 that is less than the current step 20. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-08-22 12:15:57,815 train_utils.py: 271: Val Epoch: [2][0/5] | Batch Time: 12.89 (12.89) | Data Time: 12.32 (12.32) | Mem (GB): 3.00 (3.00/3.00) | Time Elapsed: 00d 00h 03m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_vos_loss: 4.19e+00 (4.19e+00)\n",
      "INFO 2025-08-22 12:16:21,568 trainer.py: 983: Estimated time remaining: 00d 00h 09m\n",
      "INFO 2025-08-22 12:16:21,569 trainer.py: 926: Synchronizing meters\n",
      "INFO 2025-08-22 12:16:21,569 trainer.py: 729: Meters: {'Losses/val_all_loss': 0, 'Losses/val_vos_loss': 3.5390583992004396, 'Losses/val_vos_loss_mask': 0.04660669341683388, 'Losses/val_vos_loss_dice': 1.9703646183013916, 'Losses/val_vos_loss_iou': 0.6365588426589965, 'Losses/val_vos_loss_class': 1.1070115817801706e-06, 'Losses/val_vos_core_loss': 3.5390583992004396, 'Trainer/where': 0.27999999999999997, 'Trainer/epoch': 2, 'Trainer/steps_val': 30}\n",
      "INFO 2025-08-22 12:16:21,763 trainer.py: 580: New best val loss: 3.539058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 30. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m world_size = \u001b[32m1\u001b[39m\n\u001b[32m      8\u001b[39m main_port = \u001b[32m12355\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43msingle_proc_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmain_port\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmain_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnode_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaster_addr\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m127.0.0.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/data/arsoyd01lab/machlm03/Segmentation/MedSAM2/training/train.py:41\u001b[39m, in \u001b[36msingle_proc_run\u001b[39m\u001b[34m(local_rank, main_port, cfg, world_size, node_rank, master_addr)\u001b[39m\n\u001b[32m     38\u001b[39m     logging.info(e)\n\u001b[32m     40\u001b[39m trainer = instantiate(cfg.trainer, _recursive_=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/data/arsoyd01lab/machlm03/Segmentation/MedSAM2/training/trainer.py:530\u001b[39m, in \u001b[36mTrainer.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    528\u001b[39m             \u001b[38;5;28mself\u001b[39m.run_val()\n\u001b[32m    529\u001b[39m             \u001b[38;5;28mself\u001b[39m.epoch += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m     \u001b[38;5;28mself\u001b[39m.run_val()\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/data/arsoyd01lab/machlm03/Segmentation/MedSAM2/training/trainer.py:555\u001b[39m, in \u001b[36mTrainer.run_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    553\u001b[39m dataloader = \u001b[38;5;28mself\u001b[39m.train_dataset.get_loader(epoch=\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.epoch))\n\u001b[32m    554\u001b[39m barrier()\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m outs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Log training metrics with global step\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.log_dict(outs, step=\u001b[38;5;28mself\u001b[39m.global_step, phase=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/data/arsoyd01lab/machlm03/Segmentation/MedSAM2/training/trainer.py:773\u001b[39m, in \u001b[36mTrainer.train_epoch\u001b[39m\u001b[34m(self, train_loader)\u001b[39m\n\u001b[32m    770\u001b[39m \u001b[38;5;28mself\u001b[39m.model.train()\n\u001b[32m    771\u001b[39m end = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_time_meter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_times\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_time_meter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/data/arsoyd01lab/machlm03/Segmentation/MedSAM2/training/dataset/sam2_datasets.py:56\u001b[39m, in \u001b[36mMixedDataLoader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     52\u001b[39m dataset_idx = \u001b[38;5;28mself\u001b[39m._iter_mixing_prob.multinomial(\n\u001b[32m     53\u001b[39m     \u001b[32m1\u001b[39m, generator=\u001b[38;5;28mself\u001b[39m.random_generator\n\u001b[32m     54\u001b[39m ).item()\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_dls\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# No more iterations for this dataset, set it's mixing probability to zero and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/medsam2/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/medsam2/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/medsam2/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1443\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1442\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1444\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1445\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/medsam2/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/medsam2/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/medsam2/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from training.train import single_proc_run\n",
    "# cfg.trainer.data.train.batch_sizes = [1]\n",
    "\n",
    "# make sure config says single GPU\n",
    "cfg.launcher.gpus_per_node = 1\n",
    "cfg.launcher.num_nodes = 1\n",
    "world_size = 1\n",
    "main_port = 12355\n",
    "\n",
    "single_proc_run(\n",
    "    local_rank=0,\n",
    "    main_port=main_port,\n",
    "    cfg=cfg,\n",
    "    world_size=world_size,\n",
    "    node_rank=0,\n",
    "    master_addr=\"127.0.0.1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61460eea-e7fb-4988-934e-d556b799b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.trainer.data.val.sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e543b18-43b4-4865-9fc2-115ff9006b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/gpfs/home/machlm03/Segmentation/OAI_demo/OAI_TrainTest/cv_txt_files/train/train_fold0.txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.trainer.data.train.datasets[0].dataset.datasets[0].video_dataset.file_list_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2102f60-5bee-465d-9fa2-c401ff7148a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Starting Fold 0\n",
      "INFO 2025-08-22 12:07:49,134 train.py:  38: resolver 'get_method' is already registered\n",
      "INFO 2025-08-22 12:07:49,156 train_utils.py:  76: Setting up torch.distributed with a timeout of 30 mins\n"
     ]
    },
    {
     "ename": "InstantiationException",
     "evalue": "Error in call to target 'training.trainer.Trainer':\nValueError('trying to initialize the default process group twice!')\nfull_key: trainer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/medsam2/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92\u001b[39m, in \u001b[36m_call_target\u001b[39m\u001b[34m(_target_, _partial_, args, kwargs, full_key)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_target_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/data/arsoyd01lab/machlm03/Segmentation/MedSAM2/training/trainer.py:231\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, data, model, logging, checkpoint, max_epochs, mode, accelerator, seed_value, val_epoch_freq, distributed, cuda, env_variables, optim, optim_overrides, meters, loss)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28mself\u001b[39m._setup_device(accelerator)\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_torch_dist_and_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28mself\u001b[39m.global_step = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/data/arsoyd01lab/machlm03/Segmentation/MedSAM2/training/trainer.py:317\u001b[39m, in \u001b[36mTrainer._setup_torch_dist_and_backend\u001b[39m\u001b[34m(self, cuda_conf, distributed_conf)\u001b[39m\n\u001b[32m    311\u001b[39m     torch.backends.cudnn.allow_tf32 = (\n\u001b[32m    312\u001b[39m         cuda_conf.cudnn_allow_tf32\n\u001b[32m    313\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m cuda_conf.cudnn_allow_tf32 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    314\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m cuda_conf.allow_tf32\n\u001b[32m    315\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m \u001b[38;5;28mself\u001b[39m.rank = \u001b[43msetup_distributed_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistributed_conf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributed_conf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimeout_mins\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/data/arsoyd01lab/machlm03/Segmentation/MedSAM2/training/utils/train_utils.py:77\u001b[39m, in \u001b[36msetup_distributed_backend\u001b[39m\u001b[34m(backend, timeout_mins)\u001b[39m\n\u001b[32m     76\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSetting up torch.distributed with a timeout of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_mins\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m mins\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43mdist\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminutes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_mins\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dist.get_rank()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/medsam2/lib/python3.12/site-packages/torch/distributed/c10d_logger.py:81\u001b[39m, in \u001b[36m_exception_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/medsam2/lib/python3.12/site-packages/torch/distributed/c10d_logger.py:95\u001b[39m, in \u001b[36m_time_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _WaitCounter(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpytorch.wait_counter.c10d.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m).guard():\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     func_return = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func_return\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/medsam2/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:1632\u001b[39m, in \u001b[36minit_process_group\u001b[39m\u001b[34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001b[39m\n\u001b[32m   1631\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m GroupMember.WORLD \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1632\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mtrying to initialize the default process group twice!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1634\u001b[39m set_pytorch_distributed_envs_from_justknobs()\n",
      "\u001b[31mValueError\u001b[39m: trying to initialize the default process group twice!",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mInstantiationException\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m cfg.fold_id = fold\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Run training for this fold\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43msingle_proc_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmain_port\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmain_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnode_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaster_addr\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m127.0.0.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Finished Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/data/arsoyd01lab/machlm03/Segmentation/MedSAM2/training/train.py:40\u001b[39m, in \u001b[36msingle_proc_run\u001b[39m\u001b[34m(local_rank, main_port, cfg, world_size, node_rank, master_addr)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     38\u001b[39m     logging.info(e)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m trainer = \u001b[43minstantiate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_recursive_\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m trainer.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/medsam2/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:226\u001b[39m, in \u001b[36minstantiate\u001b[39m\u001b[34m(config, *args, **kwargs)\u001b[39m\n\u001b[32m    223\u001b[39m     _convert_ = config.pop(_Keys.CONVERT, ConvertMode.NONE)\n\u001b[32m    224\u001b[39m     _partial_ = config.pop(_Keys.PARTIAL, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minstantiate_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_recursive_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_convert_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_partial_\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m OmegaConf.is_list(config):\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m# Finalize config (convert targets to strings, merge with kwargs)\u001b[39;00m\n\u001b[32m    231\u001b[39m     config_copy = copy.deepcopy(config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/medsam2/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:347\u001b[39m, in \u001b[36minstantiate_node\u001b[39m\u001b[34m(node, convert, recursive, partial, *args)\u001b[39m\n\u001b[32m    342\u001b[39m                 value = instantiate_node(\n\u001b[32m    343\u001b[39m                     value, convert=convert, recursive=recursive\n\u001b[32m    344\u001b[39m                 )\n\u001b[32m    345\u001b[39m             kwargs[key] = _convert_node(value, convert)\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_target_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    349\u001b[39m     \u001b[38;5;66;03m# If ALL or PARTIAL non structured or OBJECT non structured,\u001b[39;00m\n\u001b[32m    350\u001b[39m     \u001b[38;5;66;03m# instantiate in dict and resolve interpolations eagerly.\u001b[39;00m\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert == ConvertMode.ALL \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    352\u001b[39m         convert \u001b[38;5;129;01min\u001b[39;00m (ConvertMode.PARTIAL, ConvertMode.OBJECT)\n\u001b[32m    353\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m node._metadata.object_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    354\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/medsam2/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:97\u001b[39m, in \u001b[36m_call_target\u001b[39m\u001b[34m(_target_, _partial_, args, kwargs, full_key)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m full_key:\n\u001b[32m     96\u001b[39m     msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mfull_key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InstantiationException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mInstantiationException\u001b[39m: Error in call to target 'training.trainer.Trainer':\nValueError('trying to initialize the default process group twice!')\nfull_key: trainer"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## to run k fold\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from training.train import single_proc_run\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "main_port = 12355\n",
    "\n",
    "for fold in range(NUM_FOLDS):\n",
    "    print(f\"🔁 Starting Fold {fold}\")\n",
    "\n",
    "    # Load base config\n",
    "    # Load base config\n",
    "    train_file = f\"/gpfs/home/machlm03/Segmentation/OAI_demo/OAI_TrainTest/cv_txt_files/train/train_fold{fold}.txt\"\n",
    "    val_file = f\"/gpfs/home/machlm03/Segmentation/OAI_demo/OAI_TrainTest/cv_txt_files/train/train_fold{fold}.txt\"\n",
    "\n",
    "    # Load base config\n",
    "    cfg = OmegaConf.load(\"/gpfs/home/machlm03/Segmentation/MedSAM2/sam2/configs/sam2.1_hiera_tiny512_FLARE_RECIST.yaml\")\n",
    "\n",
    "    # Update config for single GPU\n",
    "    cfg.launcher.gpus_per_node = 1\n",
    "    cfg.launcher.num_nodes = 1\n",
    "    world_size = 1\n",
    "\n",
    "\n",
    "\n",
    "    cfg.trainer.data.train.datasets[0].dataset.datasets[0].video_dataset.file_list_txt = train_file\n",
    "\n",
    "    cfg.trainer.data.val = {\n",
    "        \"_target_\": \"training.dataset.vos_dataset.VOSDataset\",\n",
    "        \"transforms\": cfg.vos.val_transforms,\n",
    "        \"training\": False,\n",
    "        \"video_dataset\": {\n",
    "            \"_target_\": \"training.dataset.vos_raw_dataset.NPZRawDataset\",\n",
    "            \"file_list_txt\": val_file,\n",
    "            \"folder\": \"/gpfs/home/machlm03/Segmentation/OAI_demo/OAI_TrainTest/V00_00m_MultiClass/npz/\"\n",
    "        },\n",
    "        \"sampler\": {\n",
    "            \"_target_\": \"training.dataset.vos_sampler.SequentialSampler\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Optional: tag the run with fold number\n",
    "    cfg.fold_id = fold\n",
    "\n",
    "    # Run training for this fold\n",
    "    single_proc_run(\n",
    "        local_rank=0,\n",
    "        main_port=main_port,\n",
    "        cfg=cfg,\n",
    "        world_size=world_size,\n",
    "        node_rank=0,\n",
    "        master_addr=\"127.0.0.1\",\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Finished Fold {fold}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777add3-e089-43ba-bf78-d5fec2743759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53904a7e-417f-4199-862b-fc61371282b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cfg.trainer.data.train.batch_sizes = [1]\n",
    "\n",
    "# main_port = 12355\n",
    "# single_node_runner(cfg, main_port=main_port, node_rank=0)\n",
    "cfg.trainer.data.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f9b8f-d88f-4b89-a638-5b55941766e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OmegaConf.to_container(cfg, resolve=False).keys())\n",
    "print(cfg.trainer.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09b156-9b1c-48bf-9bdb-6a2394bab7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = cfg.trainer.data.val.datasets # first vos dataset\n",
    "print(val_dataset)\n",
    "\n",
    "# from hydra.utils import instantiate\n",
    "\n",
    "# # Instantiate the dataset object\n",
    "# val_dataset_obj = instantiate(val_dataset)\n",
    "\n",
    "# print(type(val_dataset_obj))  # should be ConcatDataset\n",
    "# print(len(val_dataset_obj))   # number of samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa56d4-6207-4964-babc-e3af10b899a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = cfg.trainer.data.val.datasets['vos'][0]  # first vos dataset\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0f194-e11c-425f-acd4-ed4b6e697f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_path = \"/gpfs/home/machlm03/Segmentation/OAI_demo/images/\"\n",
    "npz_files = [f for f in os.listdir(dataset_path) if f.endswith(\".gz\")]\n",
    "\n",
    "print(f\"Found {len(npz_files)} .gz files\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f04937-32e7-4be6-b955-174bd68a8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.utils import instantiate\n",
    "\n",
    "# Test sampler instantiation\n",
    "sampler_config = {\n",
    "    '_target_': 'training.dataset.vos_sampler.RandomUniformSampler',\n",
    "    'num_frames': 4,  # replace with actual value\n",
    "    'max_num_objects': 1  # replace with actual value\n",
    "}\n",
    "\n",
    "sampler = instantiate(sampler_config)\n",
    "print(f\"Sampler type: {type(sampler)}\")\n",
    "print(f\"Has sample method: {hasattr(sampler, 'sample')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce991c-b28a-4915-aac6-f3663b37ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.trainer.data.train.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94281811-fe68-43b2-baa3-e3463cc54777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN sampler config\n",
    "train_dataset_cfg = cfg.trainer.data.train.datasets[0]['dataset']['datasets'][0]\n",
    "train_sampler_cfg = train_dataset_cfg['sampler']\n",
    "print(\"TRAIN sampler config:\")\n",
    "print(train_sampler_cfg)\n",
    "print(f\"Type: {type(train_sampler_cfg)}\")\n",
    "\n",
    "# VAL sampler config\n",
    "val_dataset_cfg = cfg.trainer.data.val.datasets['vos'][0]['datasets'][0]\n",
    "val_sampler_cfg = val_dataset_cfg['sampler']\n",
    "print(\"\\nVAL sampler config:\")\n",
    "print(val_sampler_cfg)\n",
    "print(f\"Type: {type(val_sampler_cfg)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affbba1-7099-4103-837c-7bc1c1c114cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the NPZRawDataset\n",
    "from training.dataset.vos_dataset import VOSDataset\n",
    "\n",
    "from training.dataset.vos_raw_dataset import NPZRawDataset\n",
    "\n",
    "npz_dataset = NPZRawDataset(\n",
    "    file_list_txt='/gpfs/home/machlm03/Segmentation/OAI_demo/OAI_TrainTest/cv_txt_files/train/val_fold0.txt',\n",
    "    folder='/gpfs/home/machlm03/Segmentation/OAI_demo/OAI_TrainTest/V00_00m_MultiClass/npz/'\n",
    ")\n",
    "\n",
    "# Then test the VOSDataset with it\n",
    "vos_dataset = VOSDataset(\n",
    "    transforms=None,\n",
    "    training=False,\n",
    "    video_dataset=npz_dataset,  # This is the NPZRawDataset instance\n",
    "    sampler={\n",
    "        '_target_': 'training.dataset.vos_sampler.RandomUniformSampler',\n",
    "        'num_frames': 4,\n",
    "        'max_num_objects': 1\n",
    "    },\n",
    "    multiplier=1\n",
    ")\n",
    "\n",
    "print(f\"Sampler type after init: {type(vos_dataset.sampler)}\")\n",
    "print(f\"Sampler value: {vos_dataset.sampler}\")\n",
    "\n",
    "# Test if we can get an item\n",
    "try:\n",
    "    item = vos_dataset[0]\n",
    "    print(f\"SUCCESS! Item type: {type(item)}\")\n",
    "    print(f\"Has frames: {hasattr(item, 'frames')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a423d5f2-a5f7-4cc5-8c8c-cbf51075a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.utils import instantiate\n",
    "from training.dataset.vos_dataset import VOSDataset\n",
    "# Test your fixed VOSDataset\n",
    "vos_dataset = VOSDataset(\n",
    "    transforms=None,\n",
    "    training=False,\n",
    "    video_dataset=npz_dataset,\n",
    "    sampler={\n",
    "        '_target_': 'training.dataset.vos_sampler.RandomUniformSampler',\n",
    "        'num_frames': 4,\n",
    "        'max_num_objects': 1\n",
    "    },\n",
    "    multiplier=1\n",
    ")\n",
    "\n",
    "print(\"Testing fixed VOSDataset:\")\n",
    "try:\n",
    "    item = vos_dataset[0]\n",
    "    print(f\"SUCCESS! Item type: {type(item)}\")\n",
    "    print(f\"Has frames: {hasattr(item, 'frames')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Still error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817f862-da52-4a14-bd03-eb576b7c3cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf71b22-dfa1-4f5f-8806-034fc176984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Test NPZRawDataset directly\n",
    "print(\"=== Testing NPZRawDataset directly ===\")\n",
    "try:\n",
    "    raw_item = npz_dataset[0]\n",
    "    print(f\"Raw dataset item type: {type(raw_item)}\")\n",
    "    print(f\"Raw dataset item: {raw_item}\")\n",
    "    if hasattr(raw_item, 'frames'):\n",
    "        print(f\"Number of frames: {len(raw_item.frames)}\")\n",
    "    else:\n",
    "        print(\"No frames attribute\")\n",
    "except Exception as e:\n",
    "    print(f\"NPZRawDataset error: {e}\")\n",
    "\n",
    "# 2. Test get_video method (used by VOSDataset)\n",
    "print(\"\\n=== Testing get_video method ===\")\n",
    "try:\n",
    "    video, segment_loader = npz_dataset.get_video(0)\n",
    "    print(f\"Video type: {type(video)}\")\n",
    "    print(f\"Segment loader: {segment_loader}\")\n",
    "    print(f\"Video has frames: {hasattr(video, 'frames')}\")\n",
    "except Exception as e:\n",
    "    print(f\"get_video error: {e}\")\n",
    "\n",
    "# 3. Test sampler separately\n",
    "print(\"\\n=== Testing sampler ===\")\n",
    "try:\n",
    "    video, segment_loader = npz_dataset.get_video(0)\n",
    "    sampled_frms_and_objs = vos_dataset.sampler.sample(\n",
    "        video, segment_loader, epoch=0\n",
    "    )\n",
    "    print(f\"Sampler output type: {type(sampled_frms_and_objs)}\")\n",
    "    print(f\"Sampler output: {sampled_frms_and_objs}\")\n",
    "except Exception as e:\n",
    "    print(f\"Sampler error: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4601d4a-cfa5-4bb0-a645-15c306f7eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Test the full VOSDataset pipeline\n",
    "print(\"\\n=== Testing VOSDataset construct method ===\")\n",
    "try:\n",
    "    video, segment_loader = npz_dataset.get_video(0)\n",
    "    sampled_frms_and_objs = vos_dataset.sampler.sample(video, segment_loader, epoch=0)\n",
    "    \n",
    "    # This is what happens in VOSDataset._get_datapoint()\n",
    "    datapoint = vos_dataset.construct(video, sampled_frms_and_objs, segment_loader)\n",
    "    print(f\"Datapoint type: {type(datapoint)}\")\n",
    "    print(f\"Datapoint: {datapoint}\")\n",
    "    if hasattr(datapoint, 'frames'):\n",
    "        print(f\"Datapoint has frames: {len(datapoint.frames)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Construct error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6c65c-4b7a-439f-ae74-90a93b7a6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vos_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32f52b-a0fe-48c2-81af-897e098434a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the actual transform pipeline\n",
    "print(\"=== Debug transform application ===\")\n",
    "\n",
    "# Get the raw datapoint before transforms\n",
    "video, segment_loader = npz_dataset.get_video(0)\n",
    "sampled_frms_and_objs = vos_dataset.sampler.sample(video, segment_loader, epoch=0)\n",
    "raw_datapoint = vos_dataset.construct(video, sampled_frms_and_objs, segment_loader)\n",
    "\n",
    "print(f\"Before transforms - frame data type: {type(raw_datapoint.frames[0].data)}\")\n",
    "print(f\"Number of transforms: {len(vos_dataset._transforms)}\")\n",
    "\n",
    "# Apply transforms step by step\n",
    "current_datapoint = raw_datapoint\n",
    "for i, transform in enumerate(vos_dataset._transforms):\n",
    "    print(f\"\\nApplying transform {i}: {type(transform).__name__}\")\n",
    "    try:\n",
    "        current_datapoint = transform(current_datapoint)\n",
    "        if hasattr(current_datapoint, 'frames') and len(current_datapoint.frames) > 0:\n",
    "            frame_data = current_datapoint.frames[0].data\n",
    "            print(f\"  After transform: {type(frame_data)}\")\n",
    "            if hasattr(frame_data, 'shape'):\n",
    "                print(f\"  Shape: {frame_data.shape}\")\n",
    "        else:\n",
    "            print(f\"  After transform: {current_datapoint}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Transform failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        break\n",
    "\n",
    "print(f\"\\nFinal datapoint type: {type(current_datapoint)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf3ef86-f2f3-47d9-a13b-a4d086f8a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete VOSDataset.__getitem__() method\n",
    "print(\"\\n=== Testing complete VOSDataset[0] ===\")\n",
    "try:\n",
    "    item = vos_dataset[0]\n",
    "    print(f\"SUCCESS! Final item type: {type(item)}\")\n",
    "    print(f\"Has frames: {hasattr(item, 'frames')}\")\n",
    "    if hasattr(item, 'frames'):\n",
    "        print(f\"Number of frames: {len(item.frames)}\")\n",
    "        print(f\"First frame data type: {type(item.frames[0].data)}\")\n",
    "        if hasattr(item.frames[0].data, 'shape'):\n",
    "            print(f\"First frame shape: {item.frames[0].data.shape}\")\n",
    "    \n",
    "    # Test if this works with collate function\n",
    "    print(\"\\n=== Testing collate function ===\")\n",
    "    from training.utils.data_utils import collate_fn\n",
    "    batch = [item]  # Single item batch\n",
    "    \n",
    "    collated = collate_fn(batch, dict_key='vos')\n",
    "    print(f\"Collate SUCCESS! Output type: {type(collated)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904781f5-4532-4d93-b754-022f87af115d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f8792-e9cf-42fe-8895-cee0c8ded94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam2",
   "language": "python",
   "name": "medsam2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
